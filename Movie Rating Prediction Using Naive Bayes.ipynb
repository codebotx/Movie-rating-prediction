{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Rating Prediction Using Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#hint: Consult the scikit-learn documentation to\n",
    "#      learn about what these classes do do\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we will analyze movie reviews and determine whether a movie is good or bad.  The movie reviews data has been downloaded from the [Rotten Tomatoes](http://www.rottentomatoes.com) website. It contains metadata for ~65,000 different movies\n",
    "\n",
    "Our aim here is to develop a Naive Bayes classifier to determine whether a movie is Fresh or Rotten based on the contents of the reviews given to the movie.  As usual, we'll start out with some exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Descriptive analysis\n",
    "\n",
    "Before beginning the \"real\" work, dig into the data a bit do see what you're dealing with.  Begin by loading the datasets and dropping rows that have missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Andrew L. Urban</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>A fantasy adventure that fuses Greek mythology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Louise Keller</td>\n",
       "      <td>False</td>\n",
       "      <td>Urban Cinefile</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-06</td>\n",
       "      <td>Uma Thurman as Medusa, the gorgon with a coiff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>FILMINK (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>With a top-notch cast and dazzling special eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>3.5/5</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ethan Alter</td>\n",
       "      <td>True</td>\n",
       "      <td>Hollywood Reporter</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>What's really lacking in The Lightning Thief i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rotten_tomatoes_link      critic_name  top_critic           publisher_name  \\\n",
       "0            m/0814255  Andrew L. Urban       False           Urban Cinefile   \n",
       "1            m/0814255    Louise Keller       False           Urban Cinefile   \n",
       "2            m/0814255              NaN       False      FILMINK (Australia)   \n",
       "3            m/0814255     Ben McEachen       False  Sunday Mail (Australia)   \n",
       "4            m/0814255      Ethan Alter        True       Hollywood Reporter   \n",
       "\n",
       "  review_type review_score review_date  \\\n",
       "0       Fresh          NaN  2010-02-06   \n",
       "1       Fresh          NaN  2010-02-06   \n",
       "2       Fresh          NaN  2010-02-09   \n",
       "3       Fresh        3.5/5  2010-02-09   \n",
       "4      Rotten          NaN  2010-02-10   \n",
       "\n",
       "                                      review_content  \n",
       "0  A fantasy adventure that fuses Greek mythology...  \n",
       "1  Uma Thurman as Medusa, the gorgon with a coiff...  \n",
       "2  With a top-notch cast and dazzling special eff...  \n",
       "3  Whether audiences will get behind The Lightnin...  \n",
       "4  What's really lacking in The Lightning Thief i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv('reviews.csv')\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "reviews = reviews[~reviews.review_content.isnull()]\n",
    "reviews = reviews[reviews.review_type != 'none']\n",
    "reviews = reviews[reviews.review_content.str.len() > 0]\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Basic features of the dataset\n",
    "\n",
    "Answer the following questions by having your python code directly print the answers:\n",
    "\n",
    "* How many unique reviews (quotes) are in the `reviews` dataset?\n",
    "* How many unique reviewers (critics) are in the `reviews` dataset?\n",
    "* How many unique movies are in the `reviews` dataset?\n",
    "* How many unique movies are in the *`movies`* dataset?\n",
    "* How many different publications have reviews in the `reviews` dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique reviews: 949181\n",
      "Number of unique reviewers: 11074\n",
      "Number of unique movies in 'reviews' dataset: 17697\n",
      "Number of unqiue movies in 'movies' dataset: 17106\n",
      "Number of unique publications: 17697\n"
     ]
    }
   ],
   "source": [
    "# enter your code below\n",
    "\n",
    "print(\"Number of unique reviews: {}\".format(len(reviews.review_content.unique())))\n",
    "print(\"Number of unique reviewers: {}\".format(len(reviews.critic_name.unique())))\n",
    "print(\"Number of unique movies in 'reviews' dataset: {}\".format(len(reviews.rotten_tomatoes_link.unique())))\n",
    "print(\"Number of unqiue movies in 'movies' dataset: {}\".format(len(movies.movie_title.unique())))\n",
    "print(\"Number of unique publications: {}\".format(len(reviews.rotten_tomatoes_link.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1.1**\n",
    "\n",
    "It is worth noting that the number of _unique_ quotes are different from the _total_ number of quotes, which means that there are some quotes which are _exactly the same_. For instance, the quote _'A masterpiece.'_ was used by both Ben Walters and Geoff Andrew. However, there are other quotes, such as _'It's sick, slick and sensational.'_, which are duplicates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Number of reviews per reviewer\n",
    "\n",
    "Using the `reviews` dataset, create a histogram showing the distribution of the number of reviews per reviewer.  Create the bin width (x-axis) to be 1 unit wide, and the range of the axis to be from 0 to 30.  Scale the y-axis to show the logarithm of (count of) the number of reviewers.  Label your axes and make your histogram look professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Reviewers')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews['count'] = 1\n",
    "reviewNum = reviews.groupby('critic_name').sum()['count']\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.hist(reviewNum, Color = 'lightblue', bins = 30, range = (0,30+1), align = 'left', rwidth = 0.85)\n",
    "plt.yscale('log', nonposy = 'clip')\n",
    "plt.title('Histogram of Number of Reviews Per Reviewer', fontsize = 18)\n",
    "plt.xlabel('Number of Reviews', fontsize = 18)\n",
    "plt.ylabel('Number of Reviewers', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Zoom in on a few reviewers\n",
    "Find the 30 critics with the most reviews, and list their names in a table along with (a) the name of the publication they work for, (b) the date of their first review, and (c) the date of their last review\n",
    "\n",
    "*hint: use the groupby function to do this quickly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publication</th>\n",
       "      <th>Date of First Review</th>\n",
       "      <th>Date of Last Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>critic_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Dennis Schwartz</td>\n",
       "      <td>[Dennis Schwartz Movie Reviews]</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Roger Ebert</td>\n",
       "      <td>[Chicago Sun-Times, RogerEbert.com, Ebert &amp; Ro...</td>\n",
       "      <td>1982-10-08</td>\n",
       "      <td>2020-07-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jeffrey M. Anderson</td>\n",
       "      <td>[Combustible Celluloid, San Francisco Examiner...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Brian Orndorf</td>\n",
       "      <td>[BrianOrndorf.com, FilmJerk.com, OhmyNews.com,...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>[Orlando Sentinel, Movie Nation, Tribune News ...</td>\n",
       "      <td>1995-11-22</td>\n",
       "      <td>2020-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Frank Swietek</td>\n",
       "      <td>[One Guy's Opinion]</td>\n",
       "      <td>1995-06-30</td>\n",
       "      <td>2020-09-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>David Nusair</td>\n",
       "      <td>[Reel Film Reviews, AskMen.com, About.com, rec...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Frederic and Mary Ann Brussat</td>\n",
       "      <td>[Spirituality &amp; Practice]</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Rich Cline</td>\n",
       "      <td>[Shadows on the Wall, Film Threat, Contactmusi...</td>\n",
       "      <td>2001-12-10</td>\n",
       "      <td>2020-10-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>James Berardinelli</td>\n",
       "      <td>[ReelViews]</td>\n",
       "      <td>1800-01-01</td>\n",
       "      <td>2020-09-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Ken Hanke</td>\n",
       "      <td>[Mountain Xpress (Asheville, NC)]</td>\n",
       "      <td>2002-06-04</td>\n",
       "      <td>2020-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MaryAnn Johanson</td>\n",
       "      <td>[Flick Filosopher, Film.com, The List, Apollo ...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Chris Hewitt</td>\n",
       "      <td>[St. Paul Pioneer Press, Minneapolis Star Trib...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-08-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Laura Clifford</td>\n",
       "      <td>[Reeling Reviews, rec.arts.movies.reviews]</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-09-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nell Minow</td>\n",
       "      <td>[Beliefnet, Movie Mom, Common Sense Media, Chi...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Emanuel Levy</td>\n",
       "      <td>[EmanuelLevy.Com, Variety, Screen International]</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Eric D. Snider</td>\n",
       "      <td>[EricDSnider.com, Cinematical, Film.com, eFilm...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Peter Bradshaw</td>\n",
       "      <td>[Guardian, Daily Mirror (UK), Observer (UK)]</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Matt Brunson</td>\n",
       "      <td>[Creative Loafing, Film Frenzy]</td>\n",
       "      <td>2002-06-03</td>\n",
       "      <td>2020-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Harvey S. Karten</td>\n",
       "      <td>[Compuserve, Film Journal International, Shock...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Dustin Putman</td>\n",
       "      <td>[TheFilmFile.com, TheBluFile.com, TheFrightFil...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Peter Travers</td>\n",
       "      <td>[Rolling Stone, People Magazine, ABC News]</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Scott Weinberg</td>\n",
       "      <td>[eFilmCritic.com, DVD Clinic, DVDTalk.com, FEA...</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Susan Granger</td>\n",
       "      <td>[SSG Syndicate, Modamag.com, www.susangranger....</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Tim Brayton</td>\n",
       "      <td>[Antagony &amp; Ecstasy, Alternate Ending]</td>\n",
       "      <td>2007-09-12</td>\n",
       "      <td>2020-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mark Dujsik</td>\n",
       "      <td>[Mark Reviews Movies, UR Chicago Magazine, Sci...</td>\n",
       "      <td>2002-02-26</td>\n",
       "      <td>2020-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nick Schager</td>\n",
       "      <td>[Slant Magazine, Lessons of Darkness, Cinemati...</td>\n",
       "      <td>2002-06-18</td>\n",
       "      <td>2020-10-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Matthew Turner</td>\n",
       "      <td>[ViewLondon, Digital Spy, WOW247, thelondonpap...</td>\n",
       "      <td>2002-10-30</td>\n",
       "      <td>2020-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Sean P. Means</td>\n",
       "      <td>[Salt Lake Tribune, Film.com, The Movie Cricket]</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2020-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Richard Roeper</td>\n",
       "      <td>[Ebert &amp; Roeper, Chicago Sun-Times, Richard Ro...</td>\n",
       "      <td>2001-11-27</td>\n",
       "      <td>2020-09-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Publication  \\\n",
       "critic_name                                                                        \n",
       "Dennis Schwartz                                  [Dennis Schwartz Movie Reviews]   \n",
       "Roger Ebert                    [Chicago Sun-Times, RogerEbert.com, Ebert & Ro...   \n",
       "Jeffrey M. Anderson            [Combustible Celluloid, San Francisco Examiner...   \n",
       "Brian Orndorf                  [BrianOrndorf.com, FilmJerk.com, OhmyNews.com,...   \n",
       "Roger Moore                    [Orlando Sentinel, Movie Nation, Tribune News ...   \n",
       "Frank Swietek                                                [One Guy's Opinion]   \n",
       "David Nusair                   [Reel Film Reviews, AskMen.com, About.com, rec...   \n",
       "Frederic and Mary Ann Brussat                          [Spirituality & Practice]   \n",
       "Rich Cline                     [Shadows on the Wall, Film Threat, Contactmusi...   \n",
       "James Berardinelli                                                   [ReelViews]   \n",
       "Ken Hanke                                      [Mountain Xpress (Asheville, NC)]   \n",
       "MaryAnn Johanson               [Flick Filosopher, Film.com, The List, Apollo ...   \n",
       "Chris Hewitt                   [St. Paul Pioneer Press, Minneapolis Star Trib...   \n",
       "Laura Clifford                        [Reeling Reviews, rec.arts.movies.reviews]   \n",
       "Nell Minow                     [Beliefnet, Movie Mom, Common Sense Media, Chi...   \n",
       "Emanuel Levy                    [EmanuelLevy.Com, Variety, Screen International]   \n",
       "Eric D. Snider                 [EricDSnider.com, Cinematical, Film.com, eFilm...   \n",
       "Peter Bradshaw                      [Guardian, Daily Mirror (UK), Observer (UK)]   \n",
       "Matt Brunson                                     [Creative Loafing, Film Frenzy]   \n",
       "Harvey S. Karten               [Compuserve, Film Journal International, Shock...   \n",
       "Dustin Putman                  [TheFilmFile.com, TheBluFile.com, TheFrightFil...   \n",
       "Peter Travers                         [Rolling Stone, People Magazine, ABC News]   \n",
       "Scott Weinberg                 [eFilmCritic.com, DVD Clinic, DVDTalk.com, FEA...   \n",
       "Susan Granger                  [SSG Syndicate, Modamag.com, www.susangranger....   \n",
       "Tim Brayton                               [Antagony & Ecstasy, Alternate Ending]   \n",
       "Mark Dujsik                    [Mark Reviews Movies, UR Chicago Magazine, Sci...   \n",
       "Nick Schager                   [Slant Magazine, Lessons of Darkness, Cinemati...   \n",
       "Matthew Turner                 [ViewLondon, Digital Spy, WOW247, thelondonpap...   \n",
       "Sean P. Means                   [Salt Lake Tribune, Film.com, The Movie Cricket]   \n",
       "Richard Roeper                 [Ebert & Roeper, Chicago Sun-Times, Richard Ro...   \n",
       "\n",
       "                              Date of First Review Date of Last Review  \n",
       "critic_name                                                             \n",
       "Dennis Schwartz                         2000-01-01          2020-09-29  \n",
       "Roger Ebert                             1982-10-08          2020-07-18  \n",
       "Jeffrey M. Anderson                     2000-01-01          2020-10-10  \n",
       "Brian Orndorf                           2000-01-01          2020-09-21  \n",
       "Roger Moore                             1995-11-22          2020-10-14  \n",
       "Frank Swietek                           1995-06-30          2020-09-24  \n",
       "David Nusair                            2000-01-01          2020-10-21  \n",
       "Frederic and Mary Ann Brussat           2000-01-01          2020-10-26  \n",
       "Rich Cline                              2001-12-10          2020-10-09  \n",
       "James Berardinelli                      1800-01-01          2020-09-14  \n",
       "Ken Hanke                               2002-06-04          2020-09-22  \n",
       "MaryAnn Johanson                        2000-01-01          2020-10-26  \n",
       "Chris Hewitt                            2000-01-01          2020-08-05  \n",
       "Laura Clifford                          2000-01-01          2020-09-16  \n",
       "Nell Minow                              2000-01-01          2020-08-28  \n",
       "Emanuel Levy                            2000-01-01          2020-05-31  \n",
       "Eric D. Snider                          2000-01-01          2020-05-28  \n",
       "Peter Bradshaw                          2000-01-01          2020-10-23  \n",
       "Matt Brunson                            2002-06-03          2020-10-28  \n",
       "Harvey S. Karten                        2000-01-01          2020-09-19  \n",
       "Dustin Putman                           2000-01-01          2020-02-21  \n",
       "Peter Travers                           2000-01-01          2020-09-23  \n",
       "Scott Weinberg                          2000-01-01          2020-10-19  \n",
       "Susan Granger                           2000-01-01          2020-10-05  \n",
       "Tim Brayton                             2007-09-12          2020-10-19  \n",
       "Mark Dujsik                             2002-02-26          2020-09-18  \n",
       "Nick Schager                            2002-06-18          2020-10-23  \n",
       "Matthew Turner                          2002-10-30          2020-09-29  \n",
       "Sean P. Means                           2000-01-01          2020-10-02  \n",
       "Richard Roeper                          2001-11-27          2020-09-18  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostReviews = reviewNum.sort_values(ascending = False)[0:30].index #extract top 30 critics with most reviews \n",
    "publication = pd.DataFrame(reviews.groupby('critic_name').publisher_name.unique()[mostReviews])\n",
    "lastReview = pd.DataFrame(reviews.groupby('critic_name').review_date.max()[mostReviews])\n",
    "firstReview = pd.DataFrame(reviews.groupby('critic_name').review_date.min()[mostReviews])\n",
    "df = pd.concat([publication, firstReview, lastReview], axis = 1)\n",
    "df.columns = ['Publication','Date of First Review','Date of Last Review']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1.3**\n",
    "\n",
    "It should be noted that some of the review dates are erroneous. For example, the date of James Berardinelli's first review is 1800-01-01 00:00:00, which is likely a case of missing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Ratings over time\n",
    "\n",
    "Using the `movies` dataset, create a plot that shows how the average rating per movie (rtAllCriticsRating) has evolved over time.  Do this by creating a scatterplot where the x-axis is the year in which the movie was released and the y-axis is rtAllCriticsRating.  Drop movies with 0 or unknown values to avoid biasing your results.  Then, create a scatterplot that has one <year,rating> point for each remaining movie, and pick an appropriate size/color/transparency for these points to ensure that the graph looks professional.  In other words, do not simply use the default settings, as this will produce a dense mess of dots that will be hard to interpret.  Finally, overlay on this scatterplot a line graph in orange showing how the average movie rating (the average of rtAllCriticsRating across all movies in a year) has changed over time.  Do you notice a trend?  What do you think it means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>movie_info</th>\n",
       "      <th>critics_consensus</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>genres</th>\n",
       "      <th>directors</th>\n",
       "      <th>authors</th>\n",
       "      <th>actors</th>\n",
       "      <th>original_release_date</th>\n",
       "      <th>...</th>\n",
       "      <th>production_company</th>\n",
       "      <th>tomatometer_status</th>\n",
       "      <th>tomatometer_rating</th>\n",
       "      <th>tomatometer_count</th>\n",
       "      <th>audience_status</th>\n",
       "      <th>audience_rating</th>\n",
       "      <th>audience_count</th>\n",
       "      <th>tomatometer_top_critics_count</th>\n",
       "      <th>tomatometer_fresh_critics_count</th>\n",
       "      <th>tomatometer_rotten_critics_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Percy Jackson &amp; the Olympians: The Lightning T...</td>\n",
       "      <td>Always trouble-prone, the life of teenager Per...</td>\n",
       "      <td>Though it may seem like just another Harry Pot...</td>\n",
       "      <td>PG</td>\n",
       "      <td>Action &amp; Adventure, Comedy, Drama, Science Fic...</td>\n",
       "      <td>Chris Columbus</td>\n",
       "      <td>Craig Titley, Chris Columbus, Rick Riordan</td>\n",
       "      <td>Logan Lerman, Brandon T. Jackson, Alexandra Da...</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>...</td>\n",
       "      <td>20th Century Fox</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>49.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>Spilled</td>\n",
       "      <td>53.0</td>\n",
       "      <td>254421.0</td>\n",
       "      <td>43</td>\n",
       "      <td>73</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>m/0878835</td>\n",
       "      <td>Please Give</td>\n",
       "      <td>Kate (Catherine Keener) and her husband Alex (...</td>\n",
       "      <td>Nicole Holofcener's newest might seem slight i...</td>\n",
       "      <td>R</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Nicole Holofcener</td>\n",
       "      <td>Nicole Holofcener</td>\n",
       "      <td>Catherine Keener, Amanda Peet, Oliver Platt, R...</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>...</td>\n",
       "      <td>Sony Pictures Classics</td>\n",
       "      <td>Certified-Fresh</td>\n",
       "      <td>87.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>Upright</td>\n",
       "      <td>64.0</td>\n",
       "      <td>11574.0</td>\n",
       "      <td>44</td>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>m/10</td>\n",
       "      <td>10</td>\n",
       "      <td>A successful, middle-aged Hollywood songwriter...</td>\n",
       "      <td>Blake Edwards' bawdy comedy may not score a pe...</td>\n",
       "      <td>R</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>Blake Edwards</td>\n",
       "      <td>Blake Edwards</td>\n",
       "      <td>Dudley Moore, Bo Derek, Julie Andrews, Robert ...</td>\n",
       "      <td>1979-10-05</td>\n",
       "      <td>...</td>\n",
       "      <td>Waner Bros.</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>67.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Spilled</td>\n",
       "      <td>53.0</td>\n",
       "      <td>14684.0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>m/1000013-12_angry_men</td>\n",
       "      <td>12 Angry Men (Twelve Angry Men)</td>\n",
       "      <td>Following the closing arguments in a murder tr...</td>\n",
       "      <td>Sidney Lumet's feature debut is a superbly wri...</td>\n",
       "      <td>NR</td>\n",
       "      <td>Classics, Drama</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Reginald Rose</td>\n",
       "      <td>Martin Balsam, John Fiedler, Lee J. Cobb, E.G....</td>\n",
       "      <td>1957-04-13</td>\n",
       "      <td>...</td>\n",
       "      <td>Criterion Collection</td>\n",
       "      <td>Certified-Fresh</td>\n",
       "      <td>100.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Upright</td>\n",
       "      <td>97.0</td>\n",
       "      <td>105386.0</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>m/1000079-20000_leagues_under_the_sea</td>\n",
       "      <td>20,000 Leagues Under The Sea</td>\n",
       "      <td>In 1866, Professor Pierre M. Aronnax (Paul Luk...</td>\n",
       "      <td>One of Disney's finest live-action adventures,...</td>\n",
       "      <td>G</td>\n",
       "      <td>Action &amp; Adventure, Drama, Kids &amp; Family</td>\n",
       "      <td>Richard Fleischer</td>\n",
       "      <td>Earl Felton</td>\n",
       "      <td>James Mason, Kirk Douglas, Paul Lukas, Peter L...</td>\n",
       "      <td>1954-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>Disney</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>89.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Upright</td>\n",
       "      <td>74.0</td>\n",
       "      <td>68918.0</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rotten_tomatoes_link  \\\n",
       "0                              m/0814255   \n",
       "1                              m/0878835   \n",
       "2                                   m/10   \n",
       "3                 m/1000013-12_angry_men   \n",
       "4  m/1000079-20000_leagues_under_the_sea   \n",
       "\n",
       "                                         movie_title  \\\n",
       "0  Percy Jackson & the Olympians: The Lightning T...   \n",
       "1                                        Please Give   \n",
       "2                                                 10   \n",
       "3                    12 Angry Men (Twelve Angry Men)   \n",
       "4                       20,000 Leagues Under The Sea   \n",
       "\n",
       "                                          movie_info  \\\n",
       "0  Always trouble-prone, the life of teenager Per...   \n",
       "1  Kate (Catherine Keener) and her husband Alex (...   \n",
       "2  A successful, middle-aged Hollywood songwriter...   \n",
       "3  Following the closing arguments in a murder tr...   \n",
       "4  In 1866, Professor Pierre M. Aronnax (Paul Luk...   \n",
       "\n",
       "                                   critics_consensus content_rating  \\\n",
       "0  Though it may seem like just another Harry Pot...             PG   \n",
       "1  Nicole Holofcener's newest might seem slight i...              R   \n",
       "2  Blake Edwards' bawdy comedy may not score a pe...              R   \n",
       "3  Sidney Lumet's feature debut is a superbly wri...             NR   \n",
       "4  One of Disney's finest live-action adventures,...              G   \n",
       "\n",
       "                                              genres          directors  \\\n",
       "0  Action & Adventure, Comedy, Drama, Science Fic...     Chris Columbus   \n",
       "1                                             Comedy  Nicole Holofcener   \n",
       "2                                    Comedy, Romance      Blake Edwards   \n",
       "3                                    Classics, Drama       Sidney Lumet   \n",
       "4           Action & Adventure, Drama, Kids & Family  Richard Fleischer   \n",
       "\n",
       "                                      authors  \\\n",
       "0  Craig Titley, Chris Columbus, Rick Riordan   \n",
       "1                           Nicole Holofcener   \n",
       "2                               Blake Edwards   \n",
       "3                               Reginald Rose   \n",
       "4                                 Earl Felton   \n",
       "\n",
       "                                              actors original_release_date  \\\n",
       "0  Logan Lerman, Brandon T. Jackson, Alexandra Da...            2010-02-12   \n",
       "1  Catherine Keener, Amanda Peet, Oliver Platt, R...            2010-04-30   \n",
       "2  Dudley Moore, Bo Derek, Julie Andrews, Robert ...            1979-10-05   \n",
       "3  Martin Balsam, John Fiedler, Lee J. Cobb, E.G....            1957-04-13   \n",
       "4  James Mason, Kirk Douglas, Paul Lukas, Peter L...            1954-01-01   \n",
       "\n",
       "   ...      production_company  tomatometer_status tomatometer_rating  \\\n",
       "0  ...        20th Century Fox              Rotten               49.0   \n",
       "1  ...  Sony Pictures Classics     Certified-Fresh               87.0   \n",
       "2  ...             Waner Bros.               Fresh               67.0   \n",
       "3  ...    Criterion Collection     Certified-Fresh              100.0   \n",
       "4  ...                  Disney               Fresh               89.0   \n",
       "\n",
       "  tomatometer_count  audience_status  audience_rating audience_count  \\\n",
       "0             149.0          Spilled             53.0       254421.0   \n",
       "1             142.0          Upright             64.0        11574.0   \n",
       "2              24.0          Spilled             53.0        14684.0   \n",
       "3              54.0          Upright             97.0       105386.0   \n",
       "4              27.0          Upright             74.0        68918.0   \n",
       "\n",
       "   tomatometer_top_critics_count  tomatometer_fresh_critics_count  \\\n",
       "0                             43                               73   \n",
       "1                             44                              123   \n",
       "2                              2                               16   \n",
       "3                              6                               54   \n",
       "4                              5                               24   \n",
       "\n",
       "   tomatometer_rotten_critics_count  \n",
       "0                                76  \n",
       "1                                19  \n",
       "2                                 8  \n",
       "3                                 0  \n",
       "4                                 3  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "cleanMovies = movies[~((movies.tomatometer_rating == '0') | (movies.tomatometer_rating == '\\\\N'))]\n",
    "cleanMovies.loc[:,\"tomatometer_rating\"] = pd.to_numeric(cleanMovies['tomatometer_rating'])\n",
    "avgAllMovies = cleanMovies.groupby('original_release_date').mean().tomatometer_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nan is not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5ebea73df31d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanMovies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'original_release_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcleanMovies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tomatometer_rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lightgreen'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavgAllMovies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavgAllMovies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'orange'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Movie Release Year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Average Rating Per Movie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2845\u001b[0m         \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2846\u001b[0m         plotnonfinite=plotnonfinite, **({\"data\": data} if data is not\n\u001b[1;32m-> 2847\u001b[1;33m         None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2848\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2849\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1601\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4432\u001b[0m         \u001b[1;31m# Process **kwargs to handle aliases, conflicts with explicit kwargs:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4434\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_unit_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mydata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4435\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4436\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_process_unit_info\u001b[1;34m(self, xdata, ydata, kwargs)\u001b[0m\n\u001b[0;32m   2123\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2125\u001b[1;33m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_single_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xunits'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2126\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_single_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mydata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yunits'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2127\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_process_single_axis\u001b[1;34m(data, axis, unit_name, kwargs)\u001b[0m\n\u001b[0;32m   2106\u001b[0m                 \u001b[1;31m# We only need to update if there is nothing set yet.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2107\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhave_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2108\u001b[1;33m                     \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2110\u001b[0m             \u001b[1;31m# Check for units in the kwargs, and if present update axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mupdate_units\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1491\u001b[0m         \u001b[0mneednew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1493\u001b[1;33m         \u001b[0mdefault\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1494\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\category.py\u001b[0m in \u001b[0;36mdefault_units\u001b[1;34m(data, axis)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# default_units->axis_info->convert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUnitData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\category.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\category.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;31m# OrderedDict just iterates over unique values in data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{val!r} is not a string\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvertible\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 \u001b[1;31m# this will only be called so long as convertible is True.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: nan is not a string"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (16,9))\n",
    "plt.scatter(cleanMovies['original_release_date'],cleanMovies['tomatometer_rating'], marker = 'x', c = 'lightgreen')\n",
    "plt.scatter(avgAllMovies.index, avgAllMovies, marker = 'x', c = 'orange')\n",
    "ax.set_xlabel('Movie Release Year', fontsize = 18)\n",
    "ax.set_ylabel('Average Rating Per Movie', fontsize = 18)\n",
    "ax.set_title('Average Rating Per Movie Over Time', fontsize = 18)\n",
    "ax.legend(['Average Rating For Each Movie','Average Rating For Each Year'], fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1.4**\n",
    "\n",
    "There is a general downward trend in the average movie rating given over time. It is difficult to attribute this decrease to any particular factor given the limited amount of information we have, but there might be two explanations for this trend: \n",
    "\n",
    "1. The expectations of the global audience and critics have been steadily increasing over time, leading to a decrease in average ratings. \n",
    "\n",
    "2. The quality of movies produced has indeed decline over time, leading to poorer average ratings. \n",
    "\n",
    "I would also argue that the average movie ratings obtained in the years _before_ 1980 may not be representative due to the small sample size available. As such, all inferences made from this general downward trend should only be taken with a pinch of salt or be further justified by other relevant data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Naive Bayes\n",
    "\n",
    "Now it gets fun!  You are going to use a [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) to build a prediction model for whether a review is fresh or rotten, depending on the text of the review. Review the assigned readings on Canvas, as well as the relevant lecture notes before embarking on this journey.\n",
    "\n",
    "### Using CountVectorizer\n",
    "\n",
    "One thing that may help you in the following problems is the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) object in Scikit-learn.  This will help you convert your raw text fields into \"bag of words\" vectors, i.e. a data structure that tells you how many times a particular word appears in a blurb.  Here's a simple example, make sure you understand what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text is\n",
      " machine learning rocks\n",
      "machine learning rules\n",
      "rocks rocks rules\n",
      "\n",
      "Transformed text vector is \n",
      " [[1 1 1 0]\n",
      " [1 1 0 1]\n",
      " [0 0 2 1]]\n",
      "\n",
      "Words for each feature:\n",
      "['learning', 'machine', 'rocks', 'rules']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['machine learning rocks', 'machine learning rules', 'rocks rocks rules']\n",
    "print(\"Original text is\\n\", '\\n'.join(text))\n",
    "print()\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(text)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()\n",
    "\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "print()\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create your X input and Y output\n",
    "\n",
    "Using the `reviews` dataframe, compute a pair of numerical X, Y arrays where:\n",
    "    \n",
    " * X is a `(nreview, nwords)` array. Each row corresponds to a bag-of-words representation for a single review. This will be the *input* to your model.\n",
    " * Y is a `nreview`-element 1/0 array, encoding whether a review is Fresh (1) or Rotten (0). This is the desired *output* from your model.\n",
    " \n",
    "Make sure to remove items with no review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "make_xy\n",
    "\n",
    "Build a bag-of-words training set for the review data\n",
    "\n",
    "Parameters\n",
    "-----------\n",
    "reviews : Pandas DataFrame\n",
    "    The review data from above\n",
    "    \n",
    "vectorizer : CountVectorizer object (optional)\n",
    "    A CountVectorizer object to use. If None,\n",
    "    then create and fit a new CountVectorizer.\n",
    "    Otherwise, re-fit the provided CountVectorizer\n",
    "    using the reviews data\n",
    "    \n",
    "Returns\n",
    "-------\n",
    "X : numpy array (dims: nreview, nwords)\n",
    "    Bag-of-words representation for each review.\n",
    "Y : numpy array (dims: nreview)\n",
    "    1/0 array. 1 = fresh review, 0 = rotten review\n",
    "\n",
    "Examples\n",
    "--------\n",
    "X, Y = make_xy(reviews)\n",
    "\"\"\"\n",
    "def make_xy(reviews, vectorizer=None):\n",
    "    if vectorizer == None: \n",
    "        vectorizer = CountVectorizer(min_df=0)\n",
    "    text = list(reviews.quote)\n",
    "    vectorizer.fit(text)\n",
    "    X = vectorizer.transform(text)\n",
    "    X = X.toarray()\n",
    "    \n",
    "    Y = np.asarray(reviews.fresh.map({'rotten': 0, 'fresh':1}))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (13419, 20875)\n",
      "Shape of Y: (13419,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X, Y = make_xy(reviews)\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of Y:', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test-Train split\n",
    "\n",
    "Next, randomly split the data into two groups: a training set (80%) and a validation set (20%).  You can do this manually, as you did in the prior problem set.  Or, use [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) to do this auto-magically.  See [this guide](http://scikit-learn.org/stable/modules/cross_validation.html) for a simple example of how `train_test_split` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (10735, 20875)\n",
      "Shape of X_test: (2684, 20875)\n",
      "Shape of Y_train: (10735,)\n",
      "Shape of Y_test: (2684,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 50)\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of Y_train:', Y_train.shape)\n",
    "print('Shape of Y_test:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Naive Bayes with `MultinomialNB`\n",
    "Use the training set to train a Naive Bayes classifier using the [`MultinomialNB`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    " object.  Report the accuracy of this model on both the training and testing data.  What do you observe?  Interpret these results!\n",
    "\n",
    "*Hint: This shouldn't take more than 5-10 lines of code to accomplish*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9226828132277597\n",
      "Test Accuracy: 0.7790611028315947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "fitted_model = MultinomialNB()\n",
    "fitted_model.fit(X_train, Y_train)\n",
    "train_acc = fitted_model.score(X_train, Y_train)\n",
    "test_acc = fitted_model.score(X_test, Y_test)\n",
    "print(\"Training Accuracy: {}\".format(train_acc))\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2.3**\n",
    "\n",
    "There's a significant difference between the training and test accuracies obtained from the multinomial Naive Bayes classifier, which are calculated as 92.3% and 77.9% respectively. This might indicate some level of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 (EXTRA CREDIT) Naive Bayes from Scratch!\n",
    "That was almost too easy, right?  Right.\n",
    "\n",
    "Your next mission, should you choose to accept it, is to write your own Naive Bayes classifier without relying on `MultinomialNB` or a similar pre-written package.  In addition to the lecture notes and assigned readings, I highly recommend that you review Michael Collins lecture notes on Nave Bayes before starting (available on Canvas).  \n",
    "\n",
    "**Note:**\n",
    "You should do this extra credit assignment *after* you have finished the rest of the problem set.  It is very rewarding, but can also be quite time-consuming!\n",
    "\n",
    "*Hint: You will benefit most from this exercise if you attempt to write the algorithm directly from the lecture notes.  That said, if you really get stuck, Krishnamurthy Viswanathan has written a simple version of [NB in python](http://ebiquity.umbc.edu/blogger/2010/12/07/naive-bayes-classifier-in-50-lines/), which you can peek at if you really get stuck.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "X, Y = make_xy(reviews)\n",
    "rotten = Y == 0\n",
    "fresh = ~rotten\n",
    "p_rotten = rotten.sum()/len(Y)\n",
    "p_fresh = fresh.sum()/len(Y)\n",
    "logp_given_rotten = np.log10((X[rotten].sum(axis = 0) + alpha)/len(X[rotten]))\n",
    "logp_given_rotten = np.log10(p_rotten) + logp_given_rotten.sum()\n",
    "logp_given_fresh = np.log10((X[fresh].sum(axis = 0) + alpha)/len(X[fresh]))\n",
    "logp_given_fresh = np.log(p_fresh) + logp_given_fresh.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Evaluation\n",
    "\n",
    "### 3.1 Estimate the likelihood of your data\n",
    "\n",
    "Given a fitted model, you can compute the log-likelihood of your data as a way to assess the performance of your model.  Using `fitted_model.predict_logproba`, the idea is to write a function that computes the log-likelihood of a dataset, so that we can inspect the log-likelihood of your training and testing data given your fitted model from part 2.\n",
    "\n",
    "To help you out a little bit, we'll do this part for you. But make sure you understand it, because you'll need to use this later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log likelihood of the model is: -1447.3249047146842\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "log_likelihood\n",
    "\n",
    "Compute the log likelihood of a dataset according to a bayesian classifier. \n",
    "The Log Likelihood is defined by\n",
    "\n",
    "L = Sum_fresh(logP(fresh)) + Sum_rotten(logP(rotten))\n",
    "\n",
    "Where Sum_fresh indicates a sum over all fresh reviews, \n",
    "and Sum_rotten indicates a sum over rotten reviews\n",
    "    \n",
    "Parameters\n",
    "----------\n",
    "model : Bayesian classifier\n",
    "x : (nexample, nfeature) array\n",
    "    The input data\n",
    "y : (nexample) integer array\n",
    "    Whether each review is Fresh\n",
    "\"\"\"\n",
    "\n",
    "def log_likelihood(model, x, y):\n",
    "    prob = model.predict_log_proba(x)\n",
    "    rotten = y == 0\n",
    "    fresh = ~rotten\n",
    "    return prob[rotten, 0].sum() + prob[fresh, 1].sum()\n",
    "\n",
    "# output the likelihood of your test data (example - you may need to \n",
    "# change the names of the variables below to match your code in 2.2 and 2.3\n",
    "\n",
    "print('The log likelihood of the model is:', log_likelihood(fitted_model, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cross-Validation\n",
    "\n",
    "Why do we care about the log-likelihood of our data?  You guessed it: Cross-Validation.\n",
    "\n",
    "Our classifier has a few hyperparameters. The two most important are:\n",
    "\n",
    " 1. The `min_df` keyword in `CountVectorizer`, which will ignore words which appear in fewer than `min_df` fraction of reviews. Words that appear only once or twice can lead to overfitting, since words which occur only a few times might correlate very well with Fresh/Rotten reviews by chance in the training dataset.\n",
    " \n",
    " 2. The [`alpha` keyword](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) in the Bayesian classifier is a \"smoothing parameter\" -- increasing the value decreases the sensitivity to any single feature, and tends to pull prediction probabilities closer to 50%. \n",
    "\n",
    "How are we are going to use cross-validation to tune these hyperparameters?  The objective function we want to maximize is the log-likelihood of our data.  Fill in the remaining code in this block, to loop over many values of `alpha` and `min_df` to determine\n",
    "which settings are \"best\" in the sense of maximizing the cross-validated log-likelihood.\n",
    "\n",
    "*hint: sklearn has a built-in function, `sklearn.cross_validation.cross_val_score`, that might save you a lot of time here...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for alpha = 0 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 0 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 0 and min_df = 0.001 ...\n",
      "Calculating for alpha = 0 and min_df = 0.01 ...\n",
      "Calculating for alpha = 0 and min_df = 0.1 ...\n",
      "Calculating for alpha = 0.1 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.001 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.01 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.1 ...\n",
      "Calculating for alpha = 1 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 1 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 1 and min_df = 0.001 ...\n",
      "Calculating for alpha = 1 and min_df = 0.01 ...\n",
      "Calculating for alpha = 1 and min_df = 0.1 ...\n",
      "Calculating for alpha = 5 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 5 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 5 and min_df = 0.001 ...\n",
      "Calculating for alpha = 5 and min_df = 0.01 ...\n",
      "Calculating for alpha = 5 and min_df = 0.1 ...\n",
      "Calculating for alpha = 10 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 10 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 10 and min_df = 0.001 ...\n",
      "Calculating for alpha = 10 and min_df = 0.01 ...\n",
      "Calculating for alpha = 10 and min_df = 0.1 ...\n",
      "Calculating for alpha = 50 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 50 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 50 and min_df = 0.001 ...\n",
      "Calculating for alpha = 50 and min_df = 0.01 ...\n",
      "Calculating for alpha = 50 and min_df = 0.1 ...\n",
      "Best alpha: 5\n",
      "Best min_df: 0.001\n",
      "Max Log Likelihood: -1442.4258173396206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [0, .1, 1, 5, 10, 50]\n",
    "min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "kf = KFold(n_splits = 5, random_state = 50, shuffle = True)\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "max_loglike = -np.inf\n",
    "\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:         \n",
    "        print('Calculating for alpha =', alpha, 'and min_df =', min_df,'...')\n",
    "        vectorizer = CountVectorizer(min_df = min_df)       \n",
    "        X, Y = make_xy(reviews, vectorizer)\n",
    "        fitted_model = MultinomialNB(alpha = alpha)\n",
    "        \n",
    "        list_loglike = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            fitted_model.fit(X[train_index], Y[train_index])\n",
    "            list_loglike.append(log_likelihood(fitted_model, X[test_index], Y[test_index]))\n",
    "            \n",
    "        if  np.mean(list_loglike) > max_loglike:\n",
    "            best_alpha = alpha\n",
    "            best_min_df = min_df\n",
    "            max_loglike = np.mean(list_loglike)\n",
    "\n",
    "print(\"Best alpha: {}\".format(best_alpha))\n",
    "print(\"Best min_df: {}\".format(best_min_df))\n",
    "print(\"Max Log Likelihood: {}\".format(max_loglike))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Putting it together\n",
    "\n",
    "Now that you've determined values for alpha and min_df that optimize the cross-validated log-likelihood, repeat the steps in 2.1-2.3 to train a final classifier with these parameters and re-evaluate the accuracy.  Discuss the various ways in which Cross-Validation has affected the model. Is the new model more or less accurate? Is overfitting better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7903120633442012\n",
      "Test Accuracy: 0.7380774962742176\n"
     ]
    }
   ],
   "source": [
    "X, Y = make_xy(reviews, CountVectorizer(min_df = 0.001))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 50)\n",
    "fitted_model = MultinomialNB(alpha = 5)\n",
    "fitted_model.fit(X_train, Y_train)\n",
    "train_acc = fitted_model.score(X_train, Y_train)\n",
    "test_acc = fitted_model.score(X_test, Y_test)\n",
    "print(\"Training Accuracy: {}\".format(train_acc))\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.3**\n",
    "\n",
    "Using the values which maximizes log likelihood, where alpha = 5 and min_df = 0.001, gives us a cross-validation training accuracy of 79.0% and cross-validation test accuracy of 73.8%. \n",
    "\n",
    "While the model demonstrates less overfitting now (due to the smaller differences in accuracy between training and test sets), both training and test accuracies have decreased from previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 (Extra Credit)\n",
    "\n",
    "What happens if you tried this again using a function besides the log-likelihood -- for example, the classification accuracy?  Interpret these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for alpha = 0 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 0 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 0 and min_df = 0.001 ...\n",
      "Calculating for alpha = 0 and min_df = 0.01 ...\n",
      "Calculating for alpha = 0 and min_df = 0.1 ...\n",
      "Calculating for alpha = 0.1 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.001 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.01 ...\n",
      "Calculating for alpha = 0.1 and min_df = 0.1 ...\n",
      "Calculating for alpha = 1 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 1 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 1 and min_df = 0.001 ...\n",
      "Calculating for alpha = 1 and min_df = 0.01 ...\n",
      "Calculating for alpha = 1 and min_df = 0.1 ...\n",
      "Calculating for alpha = 5 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 5 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 5 and min_df = 0.001 ...\n",
      "Calculating for alpha = 5 and min_df = 0.01 ...\n",
      "Calculating for alpha = 5 and min_df = 0.1 ...\n",
      "Calculating for alpha = 10 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 10 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 10 and min_df = 0.001 ...\n",
      "Calculating for alpha = 10 and min_df = 0.01 ...\n",
      "Calculating for alpha = 10 and min_df = 0.1 ...\n",
      "Calculating for alpha = 50 and min_df = 1e-05 ...\n",
      "Calculating for alpha = 50 and min_df = 0.0001 ...\n",
      "Calculating for alpha = 50 and min_df = 0.001 ...\n",
      "Calculating for alpha = 50 and min_df = 0.01 ...\n",
      "Calculating for alpha = 50 and min_df = 0.1 ...\n",
      "Best alpha: 1\n",
      "Best min_df: 1e-05\n",
      "Max Accuracy: 0.7711453913335218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [0, .1, 1, 5, 10, 50]\n",
    "min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "max_acc = -np.inf\n",
    "\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:         \n",
    "        print('Calculating for alpha =', alpha, 'and min_df =', min_df,'...')\n",
    "        vectorizer = CountVectorizer(min_df = min_df)       \n",
    "        X, Y = make_xy(reviews, vectorizer)\n",
    "        fitted_model = MultinomialNB(alpha = alpha)\n",
    "        acc = cross_val_score(fitted_model, X, Y, cv = 5)            \n",
    "        if  np.mean(acc) > max_acc:\n",
    "            best_alpha = alpha\n",
    "            best_min_df = min_df\n",
    "            max_acc = np.mean(acc)\n",
    "\n",
    "print(\"Best alpha: {}\".format(best_alpha))\n",
    "print(\"Best min_df: {}\".format(best_min_df))\n",
    "print(\"Max Accuracy: {}\".format(max_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.4**\n",
    "\n",
    "In this section, I repeated the simulation with classification accuracy as an evaluation method (instead of log likelihood). It appears that the best alpha and best min_df values are 1 and 1e-05 respectively, both of which are different from the optimal values obtained when the log likelihood was used as an evaluation measure. \n",
    "\n",
    "In this case, we achieved a cross-validation test accuracy of about 77.1%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Interpretation\n",
    "\n",
    "What words best predict a fresh or rotten review?  Using your classifier and the `vectorizer.get_feature_names` method, determine which words best predict a positive or negative review. Print the 10 words that best predict a \"fresh\" review, and the 10 words that best predict a \"rotten\" review. For each word, what is the model's probability of freshness if the word appears one time?\n",
    "\n",
    "#### Hints\n",
    "* In thinking about how to measure the impact of a word on freshness rating, consider computing the classification probability for a feature vector which consists of all 0s, except for a single 1. What does this probability refer to?\n",
    "* `numpy.identity` generates an identity matrix, where all values are zero except the diagonal elements which have a value of one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>difference</th>\n",
       "      <th>predict rotten</th>\n",
       "      <th>predict fresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>masterpiece</td>\n",
       "      <td>-2.323942</td>\n",
       "      <td>-2.417329</td>\n",
       "      <td>-0.093387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>delight</td>\n",
       "      <td>-2.274645</td>\n",
       "      <td>-2.372528</td>\n",
       "      <td>-0.097883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>touching</td>\n",
       "      <td>-2.039269</td>\n",
       "      <td>-2.161596</td>\n",
       "      <td>-0.122327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>superb</td>\n",
       "      <td>-1.955748</td>\n",
       "      <td>-2.088055</td>\n",
       "      <td>-0.132307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>remarkable</td>\n",
       "      <td>-1.955748</td>\n",
       "      <td>-2.088055</td>\n",
       "      <td>-0.132307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>intelligent</td>\n",
       "      <td>-1.923333</td>\n",
       "      <td>-2.059715</td>\n",
       "      <td>-0.136382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>witty</td>\n",
       "      <td>-1.905737</td>\n",
       "      <td>-2.044380</td>\n",
       "      <td>-0.138642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>stunning</td>\n",
       "      <td>-1.894688</td>\n",
       "      <td>-2.034767</td>\n",
       "      <td>-0.140080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>extraordinary</td>\n",
       "      <td>-1.894688</td>\n",
       "      <td>-2.034767</td>\n",
       "      <td>-0.140080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>complex</td>\n",
       "      <td>-1.886319</td>\n",
       "      <td>-2.027497</td>\n",
       "      <td>-0.141177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  difference  predict rotten  predict fresh\n",
       "1157    masterpiece   -2.323942       -2.417329      -0.093387\n",
       "460         delight   -2.274645       -2.372528      -0.097883\n",
       "1952       touching   -2.039269       -2.161596      -0.122327\n",
       "1833         superb   -1.955748       -2.088055      -0.132307\n",
       "1538     remarkable   -1.955748       -2.088055      -0.132307\n",
       "974     intelligent   -1.923333       -2.059715      -0.136382\n",
       "2132          witty   -1.905737       -2.044380      -0.138642\n",
       "1811       stunning   -1.894688       -2.034767      -0.140080\n",
       "643   extraordinary   -1.894688       -2.034767      -0.140080\n",
       "352         complex   -1.886319       -2.027497      -0.141177"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model on all data, and using optimal parameters for maximizing log likelihood\n",
    "vectorizer = CountVectorizer(min_df = 0.001)\n",
    "X, Y = make_xy(reviews, vectorizer)\n",
    "fitted_model = MultinomialNB(alpha = 5)\n",
    "fitted_model.fit(X,Y)\n",
    "I = np.identity(X.shape[1])\n",
    "\n",
    "#finding the classification probability for each word\n",
    "prob = fitted_model.predict_log_proba(I)\n",
    "\n",
    "#words with a large difference between these two columns mean they cnn predict \"fresh\" or \"rotten\" well. \n",
    "#A large negative difference means the word predicts \"fresh\" well. A large positive difference means the word predicts \"rotten\" well.\n",
    "\n",
    "diff = list(prob[:,0] - prob[:,1])\n",
    "combine = pd.concat([pd.DataFrame(vectorizer.get_feature_names()),pd.DataFrame(diff),pd.DataFrame(prob)], axis = 1)\n",
    "combine.columns = ['word','difference','predict rotten','predict fresh']\n",
    "combine = combine.sort_values(by = 'difference')\n",
    "combine[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>difference</th>\n",
       "      <th>predict rotten</th>\n",
       "      <th>predict fresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>tiresome</td>\n",
       "      <td>1.401149</td>\n",
       "      <td>-0.220190</td>\n",
       "      <td>-1.621339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>sadly</td>\n",
       "      <td>1.401149</td>\n",
       "      <td>-0.220190</td>\n",
       "      <td>-1.621339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>unfunny</td>\n",
       "      <td>1.432402</td>\n",
       "      <td>-0.214091</td>\n",
       "      <td>-1.646492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>dull</td>\n",
       "      <td>1.439371</td>\n",
       "      <td>-0.212751</td>\n",
       "      <td>-1.652122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>disappointment</td>\n",
       "      <td>1.470142</td>\n",
       "      <td>-0.206927</td>\n",
       "      <td>-1.677069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>bland</td>\n",
       "      <td>1.575503</td>\n",
       "      <td>-0.188058</td>\n",
       "      <td>-1.763561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>uninspired</td>\n",
       "      <td>1.614723</td>\n",
       "      <td>-0.181443</td>\n",
       "      <td>-1.796166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>pointless</td>\n",
       "      <td>1.757824</td>\n",
       "      <td>-0.159070</td>\n",
       "      <td>-1.916894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>lame</td>\n",
       "      <td>1.790614</td>\n",
       "      <td>-0.154314</td>\n",
       "      <td>-1.944928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>unfortunately</td>\n",
       "      <td>2.045506</td>\n",
       "      <td>-0.121611</td>\n",
       "      <td>-2.167117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  difference  predict rotten  predict fresh\n",
       "1937        tiresome    1.401149       -0.220190      -1.621339\n",
       "1590           sadly    1.401149       -0.220190      -1.621339\n",
       "2010         unfunny    1.432402       -0.214091      -1.646492\n",
       "531             dull    1.439371       -0.212751      -1.652122\n",
       "501   disappointment    1.470142       -0.206927      -1.677069\n",
       "198            bland    1.575503       -0.188058      -1.763561\n",
       "2011      uninspired    1.614723       -0.181443      -1.796166\n",
       "1423       pointless    1.757824       -0.159070      -1.916894\n",
       "1047            lame    1.790614       -0.154314      -1.944928\n",
       "2009   unfortunately    2.045506       -0.121611      -2.167117"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.5**\n",
    "\n",
    "**(a)** In determining the 10 words which predict \"fresh\" best, and the 10 words which predict \"rotten\" best, we need to find words with the _largest difference_ between the classification probabilities for \"rotten\" and \"fresh\". In the method shown above, a _large negative number_ (more negative) means the word predicts \"fresh\" best, while a _large positive number_ (more positive) means the word predicts \"rotten\" best. \n",
    "\n",
    "Judging from the 20 words printed out, it does seem like the Naive Bayes classifier is learning fairly well. All words in the first table can have strong positive meanings, while all words in the second table are often associated with negativity. \n",
    "\n",
    "**(b)** The probability of the model predicting \"fresh\" is given by the \"predict fresh\" column (last column) in the tables. Evidently, this value is much higher for the first table (words which predict \"fresh\" best) when compared to the second table (words which predict \"rotten\" best). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Error Analysis\n",
    "\n",
    "One of the best sources for inspiration when trying to improve a model is to look at examples where the model performs poorly.  Find 5 fresh and rotten reviews where your model performs particularly poorly. Print each review.\n",
    "\n",
    "What do you notice about these mis-predictions? Naive Bayes classifiers assume that every word affects the probability independently of other words. In what way is this a bad assumption? In your answer, report your classifier's Freshness probability for the review \"This movie is not remarkable, touching, or superb in any way\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>fresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The script, by Lasseter, Pete Docter, Andrew Stanton and Joe Ranft, is filled with clever gags that keep the two heroes at each other's throats and the plot on fast- forward.</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A gloomy special-effects extravaganza filled with grotesque images, generating fear and despair.</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Walter Matthau and Jack Lemmon are awfully good at this sort of thing.</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Just don't expect their bickering to be on the level of Neil Simon, and you won't be disappointed.</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>If you poke through Grumpy's cheap sentimentality, you'll find a worthy picture somewhere.</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                             quote  \\\n",
       "11  The script, by Lasseter, Pete Docter, Andrew Stanton and Joe Ranft, is filled with clever gags that keep the two heroes at each other's throats and the plot on fast- forward.   \n",
       "16  A gloomy special-effects extravaganza filled with grotesque images, generating fear and despair.                                                                                 \n",
       "17  Walter Matthau and Jack Lemmon are awfully good at this sort of thing.                                                                                                           \n",
       "19  Just don't expect their bickering to be on the level of Neil Simon, and you won't be disappointed.                                                                               \n",
       "22  If you poke through Grumpy's cheap sentimentality, you'll find a worthy picture somewhere.                                                                                       \n",
       "\n",
       "     fresh  \n",
       "11  fresh   \n",
       "16  rotten  \n",
       "17  fresh   \n",
       "19  fresh   \n",
       "22  fresh   "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "X, Y = make_xy(reviews, CountVectorizer(min_df = 0.001))\n",
    "fitted_model = MultinomialNB(alpha = 5)\n",
    "fitted_model.fit(X, Y)\n",
    "wrong_predictions = fitted_model.predict(X) != Y\n",
    "poor_performance = reviews.loc[list(wrong_predictions),['quote','fresh']]\n",
    "poor_performance[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.6**\n",
    "\n",
    "**(a)** A critical assumption of the Naive Bayes classifier is that each word affects the 'freshness' probability independent of other words. This is erroneous because in many cases, the meaning of a review is formed by a _group of words_, and not the meaning of any particular word alone. \n",
    "\n",
    "For example, the third row of the reviews above was falsely classified as \"rotten\" when it is actually \"fresh\". This is likely due to the presence of the word \"awfully\", which has a negative connotation when taken on its own.\n",
    "\n",
    "Similarly, in the fourth row, the occurrence of the word \"disappointed\" might have led to model misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = (list(reviews.quote) + ['This movie is not remarkable, touching, or superb in any way'])\n",
    "vectorizer = CountVectorizer(min_df=0.001)\n",
    "vectorizer.fit(text2)\n",
    "m = vectorizer.transform(text2)\n",
    "m = m.toarray()[-1,:] #extract the last line, which is a vectorized version of the desired quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted freshness for 'This movie is not remarkable, touching, or superb in any way' is : [1]\n"
     ]
    }
   ],
   "source": [
    "X, Y = make_xy(reviews, CountVectorizer(min_df = 0.001))\n",
    "fitted_model = MultinomialNB(alpha = 5)\n",
    "fitted_model.fit(X, Y)\n",
    "predicted_freshness_m = fitted_model.predict(m.reshape(1,-1))\n",
    "print('The predicted freshness for \\'This movie is not remarkable, touching, or superb in any way\\' is :', predicted_freshness_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3.6** \n",
    "\n",
    "**(b)** As expected, due to the high occurrence of positive words (when taken on its own) such as \"remarkable\", \"touching\" and \"superb\", the critic was predicted to give a \"fresh\" rating. The Naive Bayes classifier failed to account for the presence of \"not\", which essentially flips the meaning of the review completely. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
